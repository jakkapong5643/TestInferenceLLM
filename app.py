import os
import torch
import streamlit as st
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
)
from transformers.utils import is_bitsandbytes_available
from peft import PeftModel

# ================== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡πÅ‡∏Å‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Sidebar) ==================
DEFAULT_BASE_ID = "scb10x/llama3.2-typhoon2-t1-3b-research-preview"
DEFAULT_ADAPTER = "./adapter_safetylfinal"  # ‡πÉ‡∏™‡πà path ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå adapter ‡∏´‡∏£‡∏∑‡∏≠ HF repo id ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ

SYSTEM_MSG = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏Å‡∏¢‡∏®.) "
    "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
    "‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏±‡πâ‡∏ô "
    "‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡πÅ‡∏à‡πâ‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ "
    "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"
)

# ================== Sidebar ==================
st.set_page_config(page_title="Student Loan Chatbot", page_icon="ü§ñ", layout="centered")
st.title("ü§ñ Student Loan Chatbot (Streamlit)")

with st.sidebar:
    st.header("‚öôÔ∏è Settings")
    base_id = st.text_input("Base model id", value=DEFAULT_BASE_ID)
    adapter_dir = st.text_input("Adapter path or HF repo id (optional)", value=DEFAULT_ADAPTER)
    use_4bit = st.checkbox("Use 4-bit quantization (bitsandbytes)", value=True)
    max_new_tokens = st.slider("Max new tokens", 64, 4096, 1024, step=64)
    temperature = st.slider("Temperature", 0.0, 1.0, 0.2, step=0.05)
    top_p = st.slider("Top-p", 0.1, 1.0, 0.95, step=0.05)
    repetition_penalty = st.slider("Repetition penalty", 1.0, 1.5, 1.05, step=0.01)

# ================== Cache ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ==================
@st.cache_resource(show_spinner=True)
@st.cache_resource(show_spinner=True)
def load_model_and_tokenizer(base_id: str, adapter_dir: str, want_4bit: bool):
    # 1) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dtype ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
    if torch.cuda.is_available():
        use_bf16 = torch.cuda.is_bf16_supported()
        dtype = torch.bfloat16 if use_bf16 else torch.float16
    else:
        # CPU/MPS/XPU ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ bnb -> ‡πÉ‡∏ä‡πâ float32 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
        dtype = torch.float32

    # 2) ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á bitsandbytes ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
    def _bnb_ok():
        if not want_4bit:
            return False
        try:
            import bitsandbytes as bnb  # noqa
        except Exception:
            return False
        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ backend ‡∏à‡∏£‡∏¥‡∏á ‡πÜ
        if torch.cuda.is_available():
            return True
        # (‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡∏à‡∏∞‡πÉ‡∏ä‡πâ MPS/HPU/XPU/NPU/CPU+IPEX ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á backend ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô ‡∏Ñ‡πà‡∏≠‡∏¢‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏≠‡∏á)
        return False

    enable_4bit = _bnb_ok()
    quant = None
    if enable_4bit:
        from transformers import BitsAndBytesConfig
        quant = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=dtype,
        )

    tokenizer = AutoTokenizer.from_pretrained(base_id, use_fast=True, trust_remote_code=True)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 3) ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡∏î‡πâ‡∏ß‡∏¢ bnb ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ; ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ fallback ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ bnb
    def _load_base(try_quant: bool):
        kw = dict(
            pretrained_model_name_or_path=base_id,
            device_map="auto",
            torch_dtype=dtype,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        # ‡∏ö‡∏≤‡∏á env ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sdpa ‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô ‡∏•‡∏≠‡∏á‡πÉ‡∏™‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ fallback
        try:
            if try_quant and quant is not None:
                kw["quantization_config"] = quant
            base = AutoModelForCausalLM.from_pretrained(attn_implementation="sdpa", **kw)
            return base
        except Exception:
            if try_quant and quant is not None:
                # ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏î quantization ‡∏≠‡∏≠‡∏Å
                pass
            # ‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ sdpa/quantization ‡πÄ‡∏•‡∏¢
            kw.pop("quantization_config", None)
            if "attn_implementation" in kw:
                kw.pop("attn_implementation", None)
            base = AutoModelForCausalLM.from_pretrained(**kw)
            return base

    base = _load_base(try_quant=enable_4bit)

    model = base
    if adapter_dir.strip():
        try:
            model = PeftModel.from_pretrained(base, adapter_dir.strip())
        except Exception as e:
            st.warning(f"‡πÇ‡∏´‡∏•‡∏î Adapter ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({e}) -> ‡πÉ‡∏ä‡πâ base model ‡πÅ‡∏ó‡∏ô")

    model.eval()
    try:
        model.config.use_cache = True
    except Exception:
        pass

    if not enable_4bit:
        st.info("üß© bitsandbytes ‡∏ñ‡∏π‡∏Å‡∏õ‡∏¥‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡πÑ‡∏°‡πà‡∏û‡∏ö backend ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö) ‚Äî ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà quantized")

    return tokenizer, model


tokenizer, model = load_model_and_tokenizer(base_id, adapter_dir, use_4bit)

# ================== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡πá‡∏ß) ==================
def generate_answer(question: str) -> str:
    messages = [
        {"role": "system", "content": SYSTEM_MSG},
        {"role": "user", "content": question}
    ]
    try:
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    except Exception:
        prompt = f"<|system|>\n{SYSTEM_MSG}\n<|user|>\n{question}\n<|assistant|>\n"

    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=float(temperature),
            top_p=float(top_p),
            do_sample=(temperature > 0.0),
            repetition_penalty=float(repetition_penalty),
            pad_token_id=tokenizer.pad_token_id,
        )

    # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô prompt ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    gen_tokens = out[0][inputs["input_ids"].shape[-1]:]
    text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()

    # ‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏™‡∏ó‡∏µ‡πà template ‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ <|assistant|>
    if "<|assistant|>" in text:
        text = text.split("<|assistant|>")[-1].strip()
    return text

# ================== ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏ä‡∏ó ==================
if "history" not in st.session_state:
    st.session_state.history = []

# ‡∏õ‡∏∏‡πà‡∏°‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏ä‡∏ó
col1, col2 = st.columns(2)
with col1:
    if st.button("üßπ Clear chat"):
        st.session_state.history = []
with col2:
    st.caption("Tip: ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô Sidebar ‡∏ñ‡πâ‡∏≤‡∏ï‡∏≠‡∏ö‡∏ä‡πâ‡∏≤/‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ")

# ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
for msg in st.session_state.history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ‡∏ä‡πà‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡πÅ‡∏ä‡∏ó
user_input = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏° (‡∏Å‡∏¢‡∏®.) ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")
if user_input:
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    st.session_state.history.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    with st.chat_message("assistant"):
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
            answer = generate_answer(user_input)
            st.markdown(answer)
    st.session_state.history.append({"role": "assistant", "content": answer})

