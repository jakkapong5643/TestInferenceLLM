import os
import torch
import streamlit as st
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
)
from transformers.utils import is_bitsandbytes_available
from peft import PeftModel

# ================== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡πÅ‡∏Å‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Sidebar) ==================
DEFAULT_BASE_ID = "scb10x/llama3.2-typhoon2-t1-3b-research-preview"
DEFAULT_ADAPTER = ""  # ‡πÉ‡∏™‡πà path ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå adapter ‡∏´‡∏£‡∏∑‡∏≠ HF repo id ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ

SYSTEM_MSG = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏Å‡∏¢‡∏®.) "
    "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
    "‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏±‡πâ‡∏ô "
    "‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡πÅ‡∏à‡πâ‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ "
    "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"
)

# ================== Sidebar ==================
st.set_page_config(page_title="Student Loan Chatbot", page_icon="ü§ñ", layout="centered")
st.title("ü§ñ Student Loan Chatbot (Streamlit)")

with st.sidebar:
    st.header("‚öôÔ∏è Settings")
    base_id = st.text_input("Base model id", value=DEFAULT_BASE_ID)
    adapter_dir = st.text_input("Adapter path or HF repo id (optional)", value=DEFAULT_ADAPTER)
    use_4bit = st.checkbox("Use 4-bit quantization (bitsandbytes)", value=True)
    max_new_tokens = st.slider("Max new tokens", 64, 4096, 1024, step=64)
    temperature = st.slider("Temperature", 0.0, 1.0, 0.2, step=0.05)
    top_p = st.slider("Top-p", 0.1, 1.0, 0.95, step=0.05)
    repetition_penalty = st.slider("Repetition penalty", 1.0, 1.5, 1.05, step=0.01)

# ================== Cache ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ==================
@st.cache_resource(show_spinner=True)
def load_model_and_tokenizer(base_id: str, adapter_dir: str, use_4bit: bool):
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dtype ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()
    dtype = torch.bfloat16 if use_bf16 else torch.float16

    quant = None
    if use_4bit and is_bitsandbytes_available():
        quant = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=dtype,
        )

    tokenizer = AutoTokenizer.from_pretrained(base_id, use_fast=True, trust_remote_code=True)
    if tokenizer.pad_token_id is None:
        # ‡∏Å‡∏±‡∏ô error ‡πÄ‡∏ß‡∏•‡∏≤ generate ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ padding
        tokenizer.pad_token = tokenizer.eos_token

    # ‡∏ö‡∏≤‡∏á environment ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sdpa -> fallback ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    try:
        base = AutoModelForCausalLM.from_pretrained(
            base_id,
            device_map="auto",
            torch_dtype=dtype,
            trust_remote_code=True,
            attn_implementation="sdpa",
            quantization_config=quant,
        )
    except Exception:
        base = AutoModelForCausalLM.from_pretrained(
            base_id,
            device_map="auto",
            torch_dtype=dtype,
            trust_remote_code=True,
            quantization_config=quant,
        )

    model = base
    # ‡πÇ‡∏´‡∏•‡∏î PEFT adapter ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏°‡∏≤ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á path ‡πÅ‡∏•‡∏∞ HF repo id)
    if adapter_dir.strip():
        try:
            model = PeftModel.from_pretrained(base, adapter_dir.strip())
        except Exception as e:
            st.warning(f"‡πÇ‡∏´‡∏•‡∏î Adapter ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({e}) -> ‡∏à‡∏∞‡πÉ‡∏ä‡πâ base model ‡πÅ‡∏ó‡∏ô")

    model.eval()
    # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ cache ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    try:
        model.config.use_cache = True
    except Exception:
        pass

    return tokenizer, model

tokenizer, model = load_model_and_tokenizer(base_id, adapter_dir, use_4bit)

# ================== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡πá‡∏ß) ==================
def generate_answer(question: str) -> str:
    messages = [
        {"role": "system", "content": SYSTEM_MSG},
        {"role": "user", "content": question}
    ]
    try:
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    except Exception:
        prompt = f"<|system|>\n{SYSTEM_MSG}\n<|user|>\n{question}\n<|assistant|>\n"

    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=float(temperature),
            top_p=float(top_p),
            do_sample=(temperature > 0.0),
            repetition_penalty=float(repetition_penalty),
            pad_token_id=tokenizer.pad_token_id,
        )

    # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô prompt ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    gen_tokens = out[0][inputs["input_ids"].shape[-1]:]
    text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()

    # ‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏™‡∏ó‡∏µ‡πà template ‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ <|assistant|>
    if "<|assistant|>" in text:
        text = text.split("<|assistant|>")[-1].strip()
    return text

# ================== ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏ä‡∏ó ==================
if "history" not in st.session_state:
    st.session_state.history = []

# ‡∏õ‡∏∏‡πà‡∏°‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏ä‡∏ó
col1, col2 = st.columns(2)
with col1:
    if st.button("üßπ Clear chat"):
        st.session_state.history = []
with col2:
    st.caption("Tip: ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô Sidebar ‡∏ñ‡πâ‡∏≤‡∏ï‡∏≠‡∏ö‡∏ä‡πâ‡∏≤/‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ")

# ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
for msg in st.session_state.history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ‡∏ä‡πà‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡πÅ‡∏ä‡∏ó
user_input = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏° (‡∏Å‡∏¢‡∏®.) ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")
if user_input:
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    st.session_state.history.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    with st.chat_message("assistant"):
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
            answer = generate_answer(user_input)
            st.markdown(answer)
    st.session_state.history.append({"role": "assistant", "content": answer})
