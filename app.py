import os
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ------------------ UI: Sidebar controls ------------------
st.set_page_config(page_title="Student Loan Chatbot (Typhoon T1 3B)", page_icon="üí¨", layout="wide")
st.title("üí¨ ‡∏Å‡∏¢‡∏®. Chatbot ‚Äî Typhoon T1 3B (PEFT optional)")

with st.sidebar:
    st.subheader("‚öôÔ∏è Settings")
    base_id = st.text_input(
        "Base model",
        value="scb10x/llama3.2-typhoon2-t1-3b-research-preview",
        help="Hugging Face repo id ‡∏Ç‡∏≠‡∏á base model"
    )
    use_adapter = st.checkbox("‡πÉ‡∏ä‡πâ LoRA Adapter", value=True)
    adapter_dir = st.text_input(
        "Adapter directory",
        value=".\typhoon-t1-3b-qlora\adapter_safety\final",
        help="‡∏û‡∏≤‡∏ò‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå adapter ‡∏ó‡∏µ‡πà‡∏°‡∏µ adapter_config.json/adapter_model.bin"
    )
    system_msg = st.text_area(
        "System message",
        value=(
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏Å‡∏¢‡∏®.) "
            "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
            "‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏±‡πâ‡∏ô "
            "‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡πÅ‡∏à‡πâ‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ "
            "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"
        ),
        height=140,
    )
    max_new_tokens = st.slider("Max new tokens", 2048, 4096, 512)
    temperature = st.slider("Temperature", 0.0, 1.5, 0.3, 0.05)
    top_p = st.slider("Top-p", 0.1, 1.0, 0.95, 0.01)
    do_sample = temperature > 0.0

    st.markdown("---")
    if st.button("üßπ ‡∏•‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"):
        st.session_state["messages"] = []

# ------------------ Cache: load tokenizer/model ------------------
@st.cache_resource(show_spinner=True)
def load_model_and_tokenizer(base_id: str, adapter_dir: str | None, use_adapter: bool):
    # ‡πÉ‡∏ä‡πâ BF16 ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ GPU ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ 4-bit ‡πÉ‡∏î‡πÜ
    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()
    dtype = torch.bfloat16 if use_bf16 else torch.float16

    tokenizer = AutoTokenizer.from_pretrained(base_id, use_fast=True, trust_remote_code=True)
    if tokenizer.pad_token is None and tokenizer.eos_token is not None:
        tokenizer.pad_token = tokenizer.eos_token

    base = AutoModelForCausalLM.from_pretrained(
        base_id,
        device_map="auto",         # ‡πÇ‡∏¢‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô GPU ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        torch_dtype=dtype,
        trust_remote_code=True,
    )

    model = base
    if use_adapter and adapter_dir and os.path.isdir(adapter_dir):
        # ‡πÇ‡∏´‡∏•‡∏î LoRA ‡πÅ‡∏•‡πâ‡∏ß merge ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡∏≠‡∏ô infer)
        model = PeftModel.from_pretrained(base, adapter_dir)
        try:
            model = model.merge_and_unload()  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô base ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà merge ‡πÅ‡∏•‡πâ‡∏ß
        except Exception:
            # ‡∏ñ‡πâ‡∏≤ merge ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡πá‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö‡∏ï‡∏¥‡∏î adapter ‡πÑ‡∏õ
            pass

    model.eval()
    if hasattr(model.config, "use_cache"):
        model.config.use_cache = True

    return tokenizer, model

tokenizer, model = load_model_and_tokenizer(
    base_id=base_id,
    adapter_dir=adapter_dir,
    use_adapter=use_adapter
)

# ------------------ Session state for chat ------------------
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏ä‡∏ó
for m in st.session_state["messages"]:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ------------------ Chat input ------------------
user_input = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏¢‡∏®. ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‚Ä¶")
if user_input:
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    st.session_state["messages"].append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏î‡πâ‡∏ß‡∏¢ chat template (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    messages = [{"role": "system", "content": system_msg}]
    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô OOM
    history_tail = st.session_state["messages"][-8:]  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 8 turn ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    messages.extend(history_tail)

    try:
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    except Exception:
        # fallback template
        prompt = (
            f"<|system|>\n{system_msg}\n"
            + "".join([f"<|{m['role']}|>\n{m['content']}\n" for m in history_tail])
            + "<|assistant|>\n"
        )

    # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡πÅ‡∏•‡∏∞‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ device ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with st.chat_message("assistant"):
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏≠‡∏ö‚Ä¶"):
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    repetition_penalty=1.05,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )

            # ‡∏ï‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏• generate ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
            gen_tokens = outputs[0][inputs["input_ids"].shape[-1]:]
            text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()
            if not text:
                # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ö‡∏≤‡∏á‡∏£‡∏∏‡πà‡∏ô‡∏î‡∏µ‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤‡∏á ‡∏•‡∏≠‡∏á‡∏î‡∏µ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡∏∞ split
                full = tokenizer.decode(outputs[0], skip_special_tokens=True)
                if "<|assistant|>" in full:
                    text = full.split("<|assistant|>")[-1].strip()
                else:
                    text = full.strip()

            st.markdown(text)
            st.session_state["messages"].append({"role": "assistant", "content": text})

# ------------------ Footer ------------------
st.caption(
    "‡πÇ‡∏°‡πÄ‡∏î‡∏•: **{}**{} ‚Ä¢ FP16/BF16 (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ 4-bit)".format(
        base_id, " + LoRA" if use_adapter else ""
    )
)
